{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import numpy as np\n",
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLinks():\n",
    "    \n",
    "    daily_links = []\n",
    "    for link in container[0].find_all(\"a\"):\n",
    "        if link['href'][37:41] == \"2020\":\n",
    "            resp = requests.get(link['href'])\n",
    "            soup = bs(resp.__dict__['_content'], \"html5lib\")\n",
    "            daily_links.append(soup.select(\"[class~=ui-state-default]\"))\n",
    "            \n",
    "    news_links = []\n",
    "    for i in range(len(daily_links)):\n",
    "        for j in range(len(daily_links[i])):\n",
    "            web_link = daily_links[i][j]['href']\n",
    "            s = bs(requests.get(web_link).__dict__['_content'], \"html5lib\")\n",
    "            news_links.append(s.select(\"[class~=archive-list]\"))\n",
    "            \n",
    "    links = []\n",
    "    for i in range(len(news_links)):\n",
    "        for j in range(len(news_links[i])):\n",
    "            for k in range(len(np.array(news_links[i][j].find_all('li')))):\n",
    "                links.append(np.array(news_links[i][j].find_all('li'))[k][1]['href'])\n",
    "                \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.thehindu.com/archive/\"\n",
    "html = requests.get(url)\n",
    "soup = bs(html.__dict__['_content'], \"html5lib\")\n",
    "container = soup.select(\"#archiveWebContainer\")\n",
    "\n",
    "links = extractLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for i in range(10):#len(links)):\n",
    "    resp = requests.get(links[i])\n",
    "    soup = bs(resp.__dict__['_content'], \"html5lib\")\n",
    "    \n",
    "    article=Article(links[i], language=\"en\")\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "    \n",
    "    data.append([{\"Title\" : article.title, \"Text\" : article.text, \"publishDate\" : article.publish_date}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
