{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Labelling_Object_Model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"M3fzr-SvZ9yi","colab_type":"text"},"source":["### Creating a label generator class\n","\n","Purpose of this Notebook :\n","*  Reuse the labelling technique for every dataset\n","*  Easy to test for any extensions/changes/improvements\n","\n","**Labelling Technique Credits : Harshita**"]},{"cell_type":"code","metadata":{"id":"DJU-pWVSn58B","colab_type":"code","outputId":"ff02e631-8765-429e-f1d7-737e0b12daa1","colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","from collections import Counter\n","# ensure cleaning.py is in the same directory\n","# from cleaning import clean_text\n","import nltk\n","nltk.download()\n","from nltk import word_tokenize\n","from nltk.corpus import wordnet as wn\n","from nltk.corpus import stopwords\n","STOPWORDS =  stopwords.words('english') + ['twitter','com']\n","from nltk.util import ngrams  \n","from heapq import nlargest\n","import collections\n","import re"],"execution_count":0,"outputs":[{"output_type":"stream","text":["NLTK Downloader\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pvlla9YHaSp7","colab_type":"text"},"source":["### Labelse generator class. One object of it can be used per dataset to generate labels for it"]},{"cell_type":"code","metadata":{"id":"165eGenLqYjf","colab_type":"code","colab":{}},"source":["class label_generator:\n","    def __init__(self,model,dataset,data_col,no_top_words):\n","        self.data = dataset\n","        self.col = data_col\n","        self.model = model\n","        self.no_top_words = no_top_words\n","\n","    def preprocess(self):\n","        # Drop NaN values and reset index\n","        self.data = self.data.dropna(subset=['text']).reset_index(drop=True)\n","        # Clean text\n","        self.clean_text()\n","        # Generate features\n","        self.all_text = ' '.join(str(word) for word in self.data[self.col].values) \n","        self.tokens = word_tokenize(self.all_text)\n","        self.vectorizer = TfidfVectorizer(max_features = 5000, ngram_range=(1,2))\n","        self.tf = self.vectorizer.fit_transform(self.data[self.col]).toarray()\n","        self.tf_feature_names = self.vectorizer.get_feature_names()\n","    \n","  \n","    def remove_URL(self,text):\n","        self.url_pattern = re.compile(r'https?://\\S+|www\\.\\S+|pic\\.twitter\\S+')\n","        return self.url_pattern.sub(r'', text)\n","    \n","    def lemmatize_text(self,text):\n","        return \" \".join([wn.morphy(word) if wn.morphy(word) != None \n","                         else word \n","                         for word in text.split()])\n","\n","    def get_topics(self):\n","        self.preprocess()\n","        self.topic_dict = {}\n","        self.model.fit(self.tf)\n","        for topic_idx, topic in enumerate(self.model.components_):\n","            self.topic_dict[\"Topic %d words\" % (topic_idx)] = ['{}'.format(self.tf_feature_names[i]) \n","                                                         for i in topic.argsort()[:-self.no_top_words - 1:-1]]\n","        return pd.DataFrame(self.topic_dict)\n","\n","    \n","    def clean_text(self):\n","        # print(\"Cleaning text and adding column 'processed_text'\")\n","        self.data['processed_text'] = self.data[self.col]\n","        # Converting to lower case\n","        self.data['processed_text'] = self.data['processed_text'].str.lower()\n","        # Removing /n characters\n","        self.data['processed_text'] = self.data['processed_text'].apply(lambda x: x.replace('\\n', ' '))\n","        # Removing urls\n","        self.data['processed_text'] = self.data['processed_text'].apply(lambda text: self.remove_URL(text))\n","        # Removing the stopwords\n","        self.data['processed_text'] = self.data['processed_text'].apply(lambda text: \" \".join([word for word in str(text).split() \n","                                                                                               if word not in STOPWORDS]))\n","        # Lemmatization of text\n","        self.data['processed_text'] = self.data['processed_text'].apply(lambda text: self.lemmatize_text(text))\n","\n","        self.col = 'processed_text'\n","        self.data.drop_duplicates(subset=self.col,inplace=True)\n","        self.data.reset_index(drop=True,inplace=True)\n","\n","                                                                        \n","    def get_binary_labeled_dataset(self):\n","        \n","        self.topics = self.get_topics()\n","        self.dv_topics = self.topics.columns[0]\n","        self.other_topics = self.topics.columns[1]\n","        self.data['label'] = ['']*len(self.data)\n","        \n","        # DV Relationships \n","        t1list = ['husband','wife','daughter','father','relative','mother','sister','uncle','grandfather','neighbour', \\\n","                  'parent','child','cousin','inlaw','in-law','boyfriend','marital','domestic','partner','family','maid', \\\n","                  'housemaid','gay','ex','liquor']\n","        \n","        # DV actions and keywords\n","        t2list = ['abuse','violence','domestic','sexual','harass','assault','bitch','torture','rape','beat','dowry','porn', \\\n","                  'acidattack','lockdown','molest','metoo','slap','fuck','mental','physical','threat', 'push','pull','murder', \\\n","                  'body','opress','force','bruise','slut','scar', 'threaten','burn','toxic','cheat','verbal','blackmail','sos', \\\n","                  'helpline','help']\n","        \n","        # Evident harassment (must be present)\n","        t3list = ['harass','sexual','physical','rape','shame','abuse','case','help','helpline','sos','save','police']\n","        \n","        # Awareness and opinions\n","        t4list = ['prevent','law','aware','survey','webinar','initiative','pandemic','commission','guide','animal','govt', \\\n","                  'article','responsibility','delete','work','tiktok','bantiktok','content','tik','tok','action','equality', \\\n","                  'politics','employee','company','right','movie','feminism','film','online','party','cyclone']\n","\n","        for i in range(len(self.data)):\n","            if any(word in self.data[self.col][i] for word in t4list) \\\n","            and self.data.label[i] == '':\n","                self.data.label[i] = 'NO_DV'\n","            \n","            elif any(word in self.data[self.col][i] for word in t1list) \\\n","            and any(word in self.data[self.col][i] for word in t2list) \\\n","            and any(word in self.data[self.col][i] for word in t3list) \\\n","            and self.data.label[i] == '':\n","                self.data.label[i] = 'DV'\n","\n","        return self.data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v21evDKJ7cAb","colab_type":"code","colab":{}},"source":["def get_labelled_df(PATH,col,number_of_topics,no_of_top_words):\n","  data = pd.read_csv(PATH)\n","  model = LatentDirichletAllocation(n_components = number_of_topics, random_state = 42)\n","  labelizer = label_generator(model=model,dataset=data,data_col=col,no_top_words=no_of_top_words)\n","  \n","  return labelizer.get_binary_labeled_dataset(),labelizer.get_topics(),labelizer.topics"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WhjaLW-7Z5p6","colab_type":"text"},"source":["## To create labels and get the topics used to create labels, call the function \n","\n","    get_labelled_df(PATH_TO_FILE,NO_OF_TOPICS,NO_OF_TOP_WORDS)\n","\n","*  PATH_TO_FILE : To retrieve the dataset\n","*  NO_OF_TOPICS : Number of classes we need (In this case 2 i.e, DV and NO_DV)\n","*  NO_OF_TOP_WORDS : Number of words considered to create each label\n"]},{"cell_type":"code","metadata":{"id":"uGOlDal0XUOL","colab_type":"code","outputId":"330b71f7-181e-4a45-fcb2-62d2c3d5a087","colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["df,_,topics = get_labelled_df(\"/content/drive/My Drive/Omdena/twitter/more-tweets-extracted.csv\",'text',2,20)\n","df"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:63: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:60: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"8t5LiQVnXbkT","colab_type":"code","outputId":"bfa60140-6ca5-4f43-b9a8-748cde894f1c","colab":{"base_uri":"https://localhost:8080/","height":704}},"source":["df"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>screen_name</th>\n","      <th>tweet_id</th>\n","      <th>tweet_url</th>\n","      <th>timestamp</th>\n","      <th>text</th>\n","      <th>hashtags</th>\n","      <th>has_media</th>\n","      <th>img_urls</th>\n","      <th>video_url</th>\n","      <th>user_location</th>\n","      <th>processed_text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1139435031204548608</td>\n","      <td>zkwnsaari</td>\n","      <td>1262868325803024389</td>\n","      <td>/zkwnsaari/status/1262868325803024389</td>\n","      <td>2020-05-19 22:10:46</td>\n","      <td>Cornering men with the phrase \"sexual harassme...</td>\n","      <td>[]</td>\n","      <td>0.0</td>\n","      <td>[]</td>\n","      <td>NaN</td>\n","      <td>Perak, Malaysia</td>\n","      <td>cornering men with the phrase \"sexual harassme...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1262662724628238338</td>\n","      <td>anuglywoman1</td>\n","      <td>1262856464651243525</td>\n","      <td>/anuglywoman1/status/1262856464651243525</td>\n","      <td>2020-05-19 21:23:38</td>\n","      <td>Sexual harassment..is to make someone fucked u...</td>\n","      <td>[]</td>\n","      <td>0.0</td>\n","      <td>[]</td>\n","      <td>NaN</td>\n","      <td>Pain</td>\n","      <td>sexual harassment..is to make someone fucked u...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>15199808</td>\n","      <td>Vidyut</td>\n","      <td>1262838343404044296</td>\n","      <td>/Vidyut/status/1262838343404044296</td>\n","      <td>2020-05-19 20:11:38</td>\n","      <td>Fellow asking about abuse women face online/st...</td>\n","      <td>[]</td>\n","      <td>0.0</td>\n","      <td>[]</td>\n","      <td>NaN</td>\n","      <td>India</td>\n","      <td>fellow asking about abuse women face online/st...</td>\n","      <td>NO_DV</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>257394747</td>\n","      <td>PramodChturvedi</td>\n","      <td>1262824245517672449</td>\n","      <td>/PramodChturvedi/status/1262824245517672449</td>\n","      <td>2020-05-19 19:15:36</td>\n","      <td>Case registered against employee of private co...</td>\n","      <td>['AndhraPradesh', 'Krishna']</td>\n","      <td>0.0</td>\n","      <td>[]</td>\n","      <td>NaN</td>\n","      <td>Hyderabad, New Delhi, Ballia</td>\n","      <td>case registered against employee of private co...</td>\n","      <td>NO_DV</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2247560024</td>\n","      <td>FeminismInIndia</td>\n","      <td>1262816952164196355</td>\n","      <td>/FeminismInIndia/status/1262816952164196355</td>\n","      <td>2020-05-19 18:46:38</td>\n","      <td>Institutional Failures &amp; The Increasing Relian...</td>\n","      <td>[]</td>\n","      <td>0.0</td>\n","      <td>[]</td>\n","      <td>NaN</td>\n","      <td>India</td>\n","      <td>institutional failures &amp; the increasing relian...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1690</th>\n","      <td>857761954018058241</td>\n","      <td>satpal13414</td>\n","      <td>1212326306739703808</td>\n","      <td>/satpal13414/status/1212326306739703808</td>\n","      <td>2020-01-01 10:54:50</td>\n","      <td>#स्वर्ण_युग\\nGolden Time Is Coming\\nThe other ...</td>\n","      <td>['स']</td>\n","      <td>1.0</td>\n","      <td>['https://pbs.twimg.com/media/ENMMVmYU0AAim4b....</td>\n","      <td>NaN</td>\n","      <td>Rewari, India</td>\n","      <td>#स्वर्ण_युग golden time is coming the other wo...</td>\n","      <td>DV</td>\n","    </tr>\n","    <tr>\n","      <th>1691</th>\n","      <td>1156956207640244224</td>\n","      <td>Mohit__solanki</td>\n","      <td>1212256040089075712</td>\n","      <td>/Mohit__solanki/status/1212256040089075712</td>\n","      <td>2020-01-01 06:15:37</td>\n","      <td>Golden Time Is Coming\\nThe other woman and gir...</td>\n","      <td>['स']</td>\n","      <td>0.0</td>\n","      <td>[]</td>\n","      <td>NaN</td>\n","      <td>Firozpur, India</td>\n","      <td>golden time is coming the other woman and girl...</td>\n","      <td>DV</td>\n","    </tr>\n","    <tr>\n","      <th>1692</th>\n","      <td>838337330322751488</td>\n","      <td>BRAJBHANDAS1234</td>\n","      <td>1212234940441493505</td>\n","      <td>/BRAJBHANDAS1234/status/1212234940441493505</td>\n","      <td>2020-01-01 04:51:46</td>\n","      <td>#स्वर्ण_युग#स्वर्ण_युग\\nGolden Time Is Coming\\...</td>\n","      <td>['स', 'स']</td>\n","      <td>1.0</td>\n","      <td>['https://pbs.twimg.com/media/ENK5O_DUwAEnmUY....</td>\n","      <td>NaN</td>\n","      <td>Gandhidham, India</td>\n","      <td>#स्वर्ण_युग#स्वर्ण_युग golden time is coming t...</td>\n","      <td>DV</td>\n","    </tr>\n","    <tr>\n","      <th>1693</th>\n","      <td>972168388490379264</td>\n","      <td>NiteshP82110245</td>\n","      <td>1212212166549331969</td>\n","      <td>/NiteshP82110245/status/1212212166549331969</td>\n","      <td>2020-01-01 03:21:16</td>\n","      <td>#HeavenOnEarth_By_SaintRampalJi\\n Time Is Comi...</td>\n","      <td>['HeavenOnEarth_By_SaintRampalJi']</td>\n","      <td>1.0</td>\n","      <td>['https://pbs.twimg.com/media/ENKkdxCVAAAxcxF....</td>\n","      <td>NaN</td>\n","      <td>मध्य प्रदेश, भारत</td>\n","      <td>#heavenonearth_by_saintrampalji  time is comin...</td>\n","      <td>DV</td>\n","    </tr>\n","    <tr>\n","      <th>1694</th>\n","      <td>2717670970</td>\n","      <td>S_R_Tunnicliffe</td>\n","      <td>1212185707508305920</td>\n","      <td>/S_R_Tunnicliffe/status/1212185707508305920</td>\n","      <td>2020-01-01 01:36:08</td>\n","      <td>This year we need some happy images of Somalia...</td>\n","      <td>[]</td>\n","      <td>1.0</td>\n","      <td>['https://pbs.twimg.com/media/ENKMeTWXsAA66Fo....</td>\n","      <td>NaN</td>\n","      <td>England, United Kingdom</td>\n","      <td>this year we need some happy images of somalia...</td>\n","      <td>DV</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1695 rows × 13 columns</p>\n","</div>"],"text/plain":["                  user_id  ...  label\n","0     1139435031204548608  ...       \n","1     1262662724628238338  ...       \n","2                15199808  ...  NO_DV\n","3               257394747  ...  NO_DV\n","4              2247560024  ...       \n","...                   ...  ...    ...\n","1690   857761954018058241  ...     DV\n","1691  1156956207640244224  ...     DV\n","1692   838337330322751488  ...     DV\n","1693   972168388490379264  ...     DV\n","1694           2717670970  ...     DV\n","\n","[1695 rows x 13 columns]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"oil0tBkoXiwf","colab_type":"code","outputId":"b1985052-5a54-4096-ce42-0a9396cf36b8","colab":{"base_uri":"https://localhost:8080/","height":677}},"source":["topics"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Topic 0 words</th>\n","      <th>Topic 1 words</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pic</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>bantiktokinindia</td>\n","      <td>of</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>bantiktok</td>\n","      <td>to</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>abuse</td>\n","      <td>and</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>attack</td>\n","      <td>sexual</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>acid attack</td>\n","      <td>in</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>of</td>\n","      <td>is</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>sexual</td>\n","      <td>for</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>promoting</td>\n","      <td>harassment</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>acid</td>\n","      <td>on</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>sexual content</td>\n","      <td>this</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>content</td>\n","      <td>it</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>promoting acid</td>\n","      <td>abuse</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>rape</td>\n","      <td>you</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>threats</td>\n","      <td>are</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>rape threats</td>\n","      <td>that</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>reasons</td>\n","      <td>violence</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>religion</td>\n","      <td>women</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>community</td>\n","      <td>sexual harassment</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>child</td>\n","      <td>not</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Topic 0 words      Topic 1 words\n","0                pic                the\n","1   bantiktokinindia                 of\n","2          bantiktok                 to\n","3              abuse                and\n","4             attack             sexual\n","5        acid attack                 in\n","6                 of                 is\n","7             sexual                for\n","8          promoting         harassment\n","9               acid                 on\n","10    sexual content               this\n","11           content                 it\n","12    promoting acid              abuse\n","13              rape                you\n","14           threats                are\n","15      rape threats               that\n","16           reasons           violence\n","17          religion              women\n","18         community  sexual harassment\n","19             child                not"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"rxgHduOwktok","colab_type":"code","outputId":"35ca64ca-3a45-4835-9f36-7690a7fab2be","colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["df,_,topics = get_labelled_df(\"/content/drive/My Drive/Omdena/reddit_domestic_violence.csv\",'title',2,20)\n","df"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:60: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:63: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"U9DlnZPDmE7D","colab_type":"code","outputId":"a0458cd8-7d25-4ab3-823f-10b0498c0c82","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["df"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>title</th>\n","      <th>score</th>\n","      <th>id</th>\n","      <th>url</th>\n","      <th>comms_num</th>\n","      <th>created</th>\n","      <th>body</th>\n","      <th>year</th>\n","      <th>month</th>\n","      <th>day</th>\n","      <th>time</th>\n","      <th>violence_type</th>\n","      <th>age</th>\n","      <th>processed_text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>There is NO ROOM for abuse.Period. Digital Pai...</td>\n","      <td>3114</td>\n","      <td>gfoc94</td>\n","      <td>https://i.redd.it/xfvcce15rhx41.jpg</td>\n","      <td>329</td>\n","      <td>1.588951e+09</td>\n","      <td>None</td>\n","      <td>2020.0</td>\n","      <td>5.0</td>\n","      <td>8.0</td>\n","      <td>15:12:12</td>\n","      <td>abuse</td>\n","      <td>NaN</td>\n","      <td>there is no room for abuse.period. digital pai...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Behind closed doors, the biggest viruses are m...</td>\n","      <td>1128</td>\n","      <td>fzzzji</td>\n","      <td>https://i.redd.it/dc1v110v0fs41.jpg</td>\n","      <td>213</td>\n","      <td>1.586738e+09</td>\n","      <td>None</td>\n","      <td>2020.0</td>\n","      <td>4.0</td>\n","      <td>13.0</td>\n","      <td>00:40:34</td>\n","      <td>abuse</td>\n","      <td>NaN</td>\n","      <td>behind closed doors, the biggest viruses are m...</td>\n","      <td>DV</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Manipur girl racially attacked, abused by loca...</td>\n","      <td>365</td>\n","      <td>gici3u</td>\n","      <td>https://newsd.in/manipur-girl-racially-attacke...</td>\n","      <td>62</td>\n","      <td>1.589323e+09</td>\n","      <td>None</td>\n","      <td>2020.0</td>\n","      <td>5.0</td>\n","      <td>12.0</td>\n","      <td>22:43:00</td>\n","      <td>abuse</td>\n","      <td>NaN</td>\n","      <td>manipur girl racially attacked, abused by loca...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Chennai man caught on camera hurling casteist ...</td>\n","      <td>402</td>\n","      <td>g30ho5</td>\n","      <td>https://www.thenewsminute.com/article/chennai-...</td>\n","      <td>114</td>\n","      <td>1.587153e+09</td>\n","      <td>None</td>\n","      <td>2020.0</td>\n","      <td>4.0</td>\n","      <td>17.0</td>\n","      <td>19:49:59</td>\n","      <td>abuse</td>\n","      <td>NaN</td>\n","      <td>chennai man caught on camera hurling casteist ...</td>\n","      <td>NO_DV</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Parental Abuse: What to do?</td>\n","      <td>31</td>\n","      <td>ggfu6u</td>\n","      <td>https://www.reddit.com/r/india/comments/ggfu6u...</td>\n","      <td>49</td>\n","      <td>1.589061e+09</td>\n","      <td>I am writing this with such a heavy heart. But...</td>\n","      <td>2020.0</td>\n","      <td>5.0</td>\n","      <td>9.0</td>\n","      <td>21:56:50</td>\n","      <td>abuse</td>\n","      <td>20.0</td>\n","      <td>parental abuse: what to do?</td>\n","      <td>DV</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>437</th>\n","      <td>532</td>\n","      <td>Do the parties signing a legal contract have c...</td>\n","      <td>1</td>\n","      <td>gbeobj</td>\n","      <td>https://www.reddit.com/r/legaladvice/comments/...</td>\n","      <td>0</td>\n","      <td>1.588352e+09</td>\n","      <td>I am a 16 yo developer and I have an idea abou...</td>\n","      <td>2020.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>16:59:43</td>\n","      <td>NaN</td>\n","      <td>16.0</td>\n","      <td>do the parties signing a legal contract have c...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>438</th>\n","      <td>533</td>\n","      <td>Is infanticide heavily biased against girls?</td>\n","      <td>8</td>\n","      <td>fdhja3</td>\n","      <td>https://www.reddit.com/r/MensRights/comments/f...</td>\n","      <td>12</td>\n","      <td>1.583376e+09</td>\n","      <td>You may know how female infanticide is notorio...</td>\n","      <td>2020.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>02:41:15</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>is infanticide heavily biased against girls?</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>439</th>\n","      <td>534</td>\n","      <td>Domestic Abuse Case in GA of USA</td>\n","      <td>1</td>\n","      <td>g9xwxd</td>\n","      <td>https://www.reddit.com/r/legaladvice/comments/...</td>\n","      <td>0</td>\n","      <td>1.588144e+09</td>\n","      <td>Hello. I want to help my mother fight against...</td>\n","      <td>2020.0</td>\n","      <td>4.0</td>\n","      <td>29.0</td>\n","      <td>07:10:06</td>\n","      <td>NaN</td>\n","      <td>15.0</td>\n","      <td>domestic abuse case in ga of usa</td>\n","      <td>DV</td>\n","    </tr>\n","    <tr>\n","      <th>440</th>\n","      <td>535</td>\n","      <td>Notarization confusion</td>\n","      <td>1</td>\n","      <td>fslkkw</td>\n","      <td>https://www.reddit.com/r/legaladvice/comments/...</td>\n","      <td>6</td>\n","      <td>1.585718e+09</td>\n","      <td>So I'm an international student from India and...</td>\n","      <td>2020.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>05:18:40</td>\n","      <td>NaN</td>\n","      <td>10.0</td>\n","      <td>notarization confusion</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>441</th>\n","      <td>536</td>\n","      <td>Company promised extra money for working more ...</td>\n","      <td>5</td>\n","      <td>g2l91y</td>\n","      <td>https://www.reddit.com/r/legaladvice/comments/...</td>\n","      <td>0</td>\n","      <td>1.587093e+09</td>\n","      <td>I am a process associate at a major BPO compan...</td>\n","      <td>2020.0</td>\n","      <td>4.0</td>\n","      <td>17.0</td>\n","      <td>03:02:44</td>\n","      <td>NaN</td>\n","      <td>30.0</td>\n","      <td>company promised extra money for working more ...</td>\n","      <td>NO_DV</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>442 rows × 16 columns</p>\n","</div>"],"text/plain":["     Unnamed: 0  ...  label\n","0             0  ...       \n","1             1  ...     DV\n","2             2  ...       \n","3             3  ...  NO_DV\n","4             5  ...     DV\n","..          ...  ...    ...\n","437         532  ...       \n","438         533  ...       \n","439         534  ...     DV\n","440         535  ...       \n","441         536  ...  NO_DV\n","\n","[442 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"GYLq3lpTmLrn","colab_type":"code","outputId":"5f478aad-5a61-4da1-e96f-8d7575438fb7","colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["df.label.value_counts()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         343\n","DV        66\n","NO_DV     33\n","Name: label, dtype: int64"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"knr8038lmYnL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}