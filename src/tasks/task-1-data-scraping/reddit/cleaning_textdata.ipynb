{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "# read in data\r\n",
                "import pandas as pd\r\n",
                "data = pd.read_csv(\"C:\\\\Users\\\\20jam\\\\Documents\\\\github\\\\my-code\\\\full_data.csv\") # do this for full_data, midCOVID_data, preCOVID_data\r\n",
                "title = data.loc[:, \"title\"]\r\n",
                "body = data.loc[:, \"selftext\"]\r\n",
                "comments = data.loc[:, \"comments\"]\r\n",
                "# print(title)\r\n",
                "# print(body)\r\n",
                "# print(comments)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "# expand contractions\r\n",
                "import contractions\r\n",
                "expanded_title = []\r\n",
                "for text in title:\r\n",
                "    expanded = []\r\n",
                "    for word in text.split():\r\n",
                "        expanded.append(contractions.fix(word))\r\n",
                "    expanded_text = \" \".join(expanded)\r\n",
                "    expanded_title.append(expanded_text)\r\n",
                "expanded_body = []\r\n",
                "for text in body:\r\n",
                "    expanded = []\r\n",
                "    for word in text.split():\r\n",
                "        expanded.append(contractions.fix(word))\r\n",
                "    expanded_text = \" \".join(expanded)\r\n",
                "    expanded_body.append(expanded_text)\r\n",
                "expanded_comments = []\r\n",
                "for text in comments:\r\n",
                "    try:\r\n",
                "        expanded = []\r\n",
                "        for word in text.split():\r\n",
                "            expanded.append(contractions.fix(word))\r\n",
                "        expanded_text = \" \".join(expanded)\r\n",
                "        expanded_comments.append(expanded_text)\r\n",
                "    except: # comments is NAN\r\n",
                "        expanded_comments.append(\"\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "# lower every case\r\n",
                "lower_title = [item.lower() for item in expanded_title]\r\n",
                "lower_body = [item.lower() for item in expanded_body]\r\n",
                "lower_comments = []\r\n",
                "for item in expanded_comments:\r\n",
                "    try:\r\n",
                "        lower_comments.append(item.lower())\r\n",
                "    except:\r\n",
                "        lower_comments.append(\"\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "# remove stopwords\r\n",
                "import nltk\r\n",
                "from nltk.corpus import stopwords\r\n",
                "nltk.download(\"stopwords\")\r\n",
                "stop_words = set(stopwords.words(\"english\"))\r\n",
                "print(stop_words)\r\n",
                "\r\n",
                "stop_title = []\r\n",
                "for text in lower_title:\r\n",
                "    text = \" \".join([word for word in text.split() if word not in stop_words])\r\n",
                "    stop_title.append(text)\r\n",
                "stop_body = []\r\n",
                "for text in lower_body:\r\n",
                "    text = \" \".join([word for word in text.split() if word not in stop_words])\r\n",
                "    stop_body.append(text)\r\n",
                "stop_comments = []\r\n",
                "for text in lower_comments:\r\n",
                "    text = \" \".join([word for word in text.split() if word not in stop_words])\r\n",
                "    stop_comments.append(text)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "[nltk_data] Downloading package stopwords to\n",
                        "[nltk_data]     C:\\Users\\20jam\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "{'by', 'shouldn', 'an', \"mightn't\", 'you', 'than', \"couldn't\", 'we', 'as', 'haven', 'not', 'during', 'will', 'after', 'they', 'a', 'both', \"weren't\", 'to', 'mightn', \"aren't\", 'doesn', 'ourselves', 'needn', \"it's\", 'the', 'ours', 'out', 'he', \"wouldn't\", 'having', 'for', 'myself', 'mustn', \"hadn't\", 'from', 'of', 'shan', 'through', 'against', 'only', 'too', 'couldn', 'yours', 'below', 'its', 'why', 'yourself', 'same', 'her', \"that'll\", 'that', 've', 'in', 'when', 'because', 'am', 'other', \"you're\", 'does', 'are', 'is', 're', 'be', 'didn', 'any', 'ma', 'nor', 'has', 'was', 'hers', 't', 'she', 'hadn', \"isn't\", 'these', 'again', 'where', 'your', 'itself', 'who', 'being', 'if', 'most', 'm', 'wasn', 'then', \"don't\", 'hasn', 'o', 'at', 'above', 'can', 'and', 's', 'but', 'such', 'won', 'aren', \"you'd\", \"mustn't\", 'were', \"you'll\", 'wouldn', 'doing', 'off', 'few', 'ain', 'before', \"you've\", 'themselves', 'my', 'here', \"didn't\", 'theirs', 'no', 'me', 'up', \"should've\", 'there', 'more', 'd', \"haven't\", 'had', 'i', 'down', 'our', 'herself', 'what', 'how', 'been', 'about', 'between', \"won't\", 'or', 'some', 'yourselves', 'him', 'them', 'just', 'll', \"shan't\", 'each', \"needn't\", 'which', 'with', 'very', 'so', 'into', 'once', 'over', 'y', 'whom', 'those', 'further', 'their', 'his', 'it', 'this', \"doesn't\", 'should', 'do', 'now', 'until', 'himself', 'did', 'on', 'don', 'have', 'isn', 'own', \"wasn't\", \"she's\", 'while', 'all', \"shouldn't\", \"hasn't\", 'under', 'weren'}\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "# repair broken unicode\r\n",
                "import ftfy\r\n",
                "ftfy_title = []\r\n",
                "for text in stop_title:\r\n",
                "    text = ftfy.fix_text(text)\r\n",
                "    ftfy_title.append(text)\r\n",
                "ftfy_body = []\r\n",
                "for text in stop_body:\r\n",
                "    text = ftfy.fix_text(text)\r\n",
                "    ftfy_body.append(text)\r\n",
                "ftfy_comments = []\r\n",
                "for text in stop_comments:\r\n",
                "    text = ftfy.fix_text(text)\r\n",
                "    ftfy_comments.append(text)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "# remove unicode\r\n",
                "import string\r\n",
                "unicode_title = []\r\n",
                "for text in ftfy_title:\r\n",
                "    text_encode = text.encode(encoding = \"ascii\", errors = \"ignore\")\r\n",
                "    text_decode = text_encode.decode()\r\n",
                "    clean_text = \" \".join([word for word in text_decode.split()]) # remove extra whitespace\r\n",
                "    unicode_title.append(clean_text)\r\n",
                "unicode_body = []\r\n",
                "for text in ftfy_body:\r\n",
                "    text_encode = text.encode(encoding = \"ascii\", errors = \"ignore\")\r\n",
                "    text_decode = text_encode.decode()\r\n",
                "    clean_text = \" \".join([word for word in text_decode.split()]) # remove extra whitespace\r\n",
                "    unicode_body.append(clean_text)\r\n",
                "unicode_comments = []\r\n",
                "for text in ftfy_comments:\r\n",
                "    text_encode = text.encode(encoding = \"ascii\", errors = \"ignore\")\r\n",
                "    text_decode = text_encode.decode()\r\n",
                "    clean_text = \" \".join([word for word in text_decode.split()]) # remove extra whitespace\r\n",
                "    unicode_comments.append(clean_text)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "# remove mentions, money sign, url, hashtags, punctuations, digits\r\n",
                "import re\r\n",
                "import string\r\n",
                "punct = set(string.punctuation)\r\n",
                "\r\n",
                "remove_title = []\r\n",
                "for text in unicode_title:\r\n",
                "    text = re.sub(\"@\\S+\", \"\", text)\r\n",
                "    text = re.sub(\"\\$\", \"\", text)\r\n",
                "    text = re.sub(r'http\\S+', \"\", text)\r\n",
                "    text = re.sub(\"#\", \"\", text)\r\n",
                "    text = \"\".join([ch for ch in text if ch not in punct])\r\n",
                "    text = re.sub(r'[0-9]', \"\", text)\r\n",
                "    remove_title.append(text)\r\n",
                "remove_body = []\r\n",
                "for text in unicode_body:\r\n",
                "    text = re.sub(\"@\\S+\", \"\", text)\r\n",
                "    text = re.sub(\"\\$\", \"\", text)\r\n",
                "    text = re.sub(r'http\\S+', \"\", text)\r\n",
                "    text = re.sub(\"#\", \"\", text)\r\n",
                "    text = \"\".join([ch for ch in text if ch not in punct])\r\n",
                "    text = re.sub(r'[0-9]', \"\", text)\r\n",
                "    remove_body.append(text)\r\n",
                "remove_comments = []\r\n",
                "for text in unicode_comments:\r\n",
                "    text = re.sub(\"@\\S+\", \"\", text)\r\n",
                "    text = re.sub(\"\\$\", \"\", text)\r\n",
                "    text = re.sub(r'http\\S+', \"\", text)\r\n",
                "    text = re.sub(\"#\", \"\", text)\r\n",
                "    text = re.sub(r'\\[removed\\]', \"\", text) # remove deleted comments\r\n",
                "    text = re.sub(r'\\[deleted\\]', \"\", text)\r\n",
                "    text = \"\".join([ch for ch in text if ch not in punct])\r\n",
                "    text = re.sub(r'[0-9]', \"\", text)\r\n",
                "    remove_comments.append(text)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "# lemmatize\r\n",
                "import nltk\r\n",
                "from nltk.stem import WordNetLemmatizer\r\n",
                "lemmatizer = WordNetLemmatizer()\r\n",
                "\r\n",
                "lemma_title = []\r\n",
                "for text in remove_title:\r\n",
                "    lemmatized = [lemmatizer.lemmatize(word) for word in text.split()]\r\n",
                "    text = \" \".join(word for word in lemmatized)\r\n",
                "    lemma_title.append(text)\r\n",
                "lemma_body = []\r\n",
                "for text in remove_body:\r\n",
                "    lemmatized = [lemmatizer.lemmatize(word) for word in text.split()]\r\n",
                "    text = \" \".join(word for word in lemmatized)\r\n",
                "    lemma_body.append(text)\r\n",
                "lemma_comments = []\r\n",
                "for text in remove_comments:\r\n",
                "    lemmatized = [lemmatizer.lemmatize(word) for word in text.split()]\r\n",
                "    text = \" \".join(word for word in lemmatized)\r\n",
                "    lemma_comments.append(text)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "# word frequency list\r\n",
                "import pandas as pd\r\n",
                "import nltk\r\n",
                "pd.Series(' '.join(lemma_body).split()).value_counts()[:50]\r\n",
                "# word frequency list can suggest words that might be common but not useful\r\n",
                "# but decision about adding it to stopword list will be supported by ngrams - for some context of usage of those words\r\n",
                "# generally, fillers, words with little sentimental value eg. greetings, sign off thanking, connectors should be in stop_words list\r\n",
                "\r\n",
                "# FROM HERE ON: EDITS DONE WITH REF TO FEEDBACK FROM EDA\r\n",
                "# remove words with aaaaaaaaaaaa\r\n",
                "# alert list from ngrams: go, singapore, feel like, go to, feel free, thank advance, long story short"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "like         13342\n",
                            "would        11184\n",
                            "time          9528\n",
                            "get           8900\n",
                            "know          8595\n",
                            "really        8539\n",
                            "year          8061\n",
                            "people        7707\n",
                            "one           7592\n",
                            "even          7368\n",
                            "want          7248\n",
                            "also          7136\n",
                            "feel          6822\n",
                            "school        6284\n",
                            "go            5994\n",
                            "day           5465\n",
                            "help          5218\n",
                            "well          5041\n",
                            "work          4992\n",
                            "singapore     4948\n",
                            "think         4924\n",
                            "student       4923\n",
                            "take          4839\n",
                            "friend        4791\n",
                            "still         4719\n",
                            "much          4691\n",
                            "need          4688\n",
                            "going         4670\n",
                            "thing         4595\n",
                            "it            4564\n",
                            "make          4424\n",
                            "u             4337\n",
                            "could         4156\n",
                            "course        4035\n",
                            "good          3967\n",
                            "life          3947\n",
                            "me            3930\n",
                            "since         3862\n",
                            "got           3766\n",
                            "study         3764\n",
                            "many          3688\n",
                            "level         3630\n",
                            "back          3555\n",
                            "first         3360\n",
                            "anyone        3334\n",
                            "question      3313\n",
                            "way           3301\n",
                            "say           3097\n",
                            "see           3029\n",
                            "may           2924\n",
                            "dtype: int64"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 9
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "print(len(lemma_title))\r\n",
                "print(len(lemma_body))\r\n",
                "print(len(lemma_comments))\r\n",
                "data[\"clean_title\"] = lemma_title\r\n",
                "data[\"clean_selftext\"] = lemma_body\r\n",
                "data[\"clean_comments\"] = lemma_comments\r\n",
                "print(data)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "12610\n",
                        "12610\n",
                        "12610\n",
                        "       Unnamed: 0          subreddit  subreddit_subscribers  \\\n",
                        "0               2       askSingapore                  44675   \n",
                        "1               4  NationalServiceSG                  11091   \n",
                        "2               5       askSingapore                  44675   \n",
                        "3               7            SGExams                  77716   \n",
                        "4               8  NationalServiceSG                  11091   \n",
                        "...           ...                ...                    ...   \n",
                        "12605       84031          singapore                 231401   \n",
                        "12606       84051          singapore                 159359   \n",
                        "12607       84081          singapore                 154905   \n",
                        "12608       84101          singapore                 195542   \n",
                        "12609       84121          singapore                 144499   \n",
                        "\n",
                        "                                                   title      id  \\\n",
                        "0      Anyone know where to get tested as an Adult fo...  ng6k8b   \n",
                        "1                           Help for adjustment disorder  mfnwal   \n",
                        "2            Where to get assessed for eating disorders?  l0ac4u   \n",
                        "3                                [RANT] Mental disorders  kzrgon   \n",
                        "4                       Downpes from adjustment disorder  kwbxjs   \n",
                        "...                                                  ...     ...   \n",
                        "12605                 If there is a vaccine, whats next?  g81nb4   \n",
                        "12606                                  Influenza Vaccine  b0hzfi   \n",
                        "12607  [ladies] HPV vaccination - y/n, experience, co...  ap5vn0   \n",
                        "12608  Planning to go back to SG this December. Polio...  dopbox   \n",
                        "12609  Just got scammed by MLM and lost money, what t...  a21gtg   \n",
                        "\n",
                        "                   author created_utc  num_comments  score  \\\n",
                        "0            summerfellxx  2021-05-19             6      8   \n",
                        "1            ElijahThor00  2021-03-29             6      5   \n",
                        "2           kanicroquette  2021-01-19             9     42   \n",
                        "3         Vivid-Wing-3460  2021-01-18             7     44   \n",
                        "4       chin_chin_daisuki  2021-01-13             8     21   \n",
                        "...                   ...         ...           ...    ...   \n",
                        "12605  QualitativeEconomy  2020-04-25             0      2   \n",
                        "12606          wintersoju  2019-03-13             8      7   \n",
                        "12607           yummydubu  2019-02-10            20      1   \n",
                        "12608   jasonrodriguez_DT  2019-10-29             6      0   \n",
                        "12609                 NaN  2018-12-01           117      1   \n",
                        "\n",
                        "                                                selftext  \\\n",
                        "0      I know mostly these tests are for children. Bu...   \n",
                        "1      People say half the war is won when you report...   \n",
                        "2      I might have an eating disorder, I don’t know ...   \n",
                        "3      After o's I have this long period of time to s...   \n",
                        "4      Facing severe adjustment problems, had a menta...   \n",
                        "...                                                  ...   \n",
                        "12605  Tbh I havent kept up with the vaccine developm...   \n",
                        "12606  Are there different brands of the vaccine avai...   \n",
                        "12607  Currently researching the vaccination and wond...   \n",
                        "12608  I checked a couple of sites including your Min...   \n",
                        "12609  I just lost $1688 to an MLM company called ICS...   \n",
                        "\n",
                        "                                                     url  upvote_ratio  \\\n",
                        "0      https://www.reddit.com/r/askSingapore/comments...          0.84   \n",
                        "1      https://www.reddit.com/r/NationalServiceSG/com...          0.86   \n",
                        "2      https://www.reddit.com/r/askSingapore/comments...          1.00   \n",
                        "3      https://www.reddit.com/r/SGExams/comments/kzrg...          0.95   \n",
                        "4      https://www.reddit.com/r/NationalServiceSG/com...          0.90   \n",
                        "...                                                  ...           ...   \n",
                        "12605  https://www.reddit.com/r/singapore/comments/g8...          0.54   \n",
                        "12606  https://www.reddit.com/r/singapore/comments/b0...          0.71   \n",
                        "12607  https://www.reddit.com/r/singapore/comments/ap...          0.73   \n",
                        "12608  https://www.reddit.com/r/singapore/comments/do...          0.56   \n",
                        "12609  https://www.reddit.com/r/singapore/comments/a2...          0.95   \n",
                        "\n",
                        "                                                comments  \\\n",
                        "0      [\"is this recent or has it been going on since...   \n",
                        "1      ['pretty long unfortunately. :( i’m currently ...   \n",
                        "2      [\"Don't self diagnose. Cheapest way: got to po...   \n",
                        "3      [\"You sound exactly like me. What I can tell y...   \n",
                        "4      ['upvote for chin_chin_daisuki', 'It can take ...   \n",
                        "...                                                  ...   \n",
                        "12605  ['What’s stopping anyone from hoarding a vacci...   \n",
                        "12606  ['Yes there are differences cos many strains o...   \n",
                        "12607  ['yes you can claim from medisave for the four...   \n",
                        "12608  [\"Not sure on what is the 'official requiremen...   \n",
                        "12609  ['Learning is part of life\\n\\nTreat that $1688...   \n",
                        "\n",
                        "                                             clean_title  \\\n",
                        "0      anyone know get tested adult auditory processi...   \n",
                        "1                               help adjustment disorder   \n",
                        "2                           get assessed eating disorder   \n",
                        "3                                   rant mental disorder   \n",
                        "4                            downpes adjustment disorder   \n",
                        "...                                                  ...   \n",
                        "12605                                       vaccine next   \n",
                        "12606                                  influenza vaccine   \n",
                        "12607  lady hpv vaccination yn experience cost medisa...   \n",
                        "12608  planning go back sg december polio vaccine req...   \n",
                        "12609                         got scammed mlm lost money   \n",
                        "\n",
                        "                                          clean_selftext  \\\n",
                        "0      know mostly test child really need know wrong ...   \n",
                        "1      people say half war report mental illness long...   \n",
                        "2      might eating disorder know want self diagnose ...   \n",
                        "3      ofs long period time sit really reflect unsolv...   \n",
                        "4      facing severe adjustment problem mental breakd...   \n",
                        "...                                                  ...   \n",
                        "12605  tbh kept vaccine development news want generat...   \n",
                        "12606  different brand vaccine available difference g...   \n",
                        "12607  currently researching vaccination wondering ma...   \n",
                        "12608  checked couple site including ministry health ...   \n",
                        "12609  lost mlm company called icsi care system feel ...   \n",
                        "\n",
                        "                                          clean_comments  \n",
                        "0      is recent going since childhood nnif recent mi...  \n",
                        "1      pretty long unfortunately currently going coun...  \n",
                        "2      do self diagnose cheapest way got polyclinic s...  \n",
                        "3      you sound exactly like me tell thought simply ...  \n",
                        "4      upvote chinchindaisuki it take month fast mont...  \n",
                        "...                                                  ...  \n",
                        "12605  what stopping anyone hoarding vaccine made dif...  \n",
                        "12606            yes difference co many strain influenza  \n",
                        "12607  yes claim medisave four strand one already sec...  \n",
                        "12608  not sure official requirement seen many friend...  \n",
                        "12609  learning part lifenntreat expensive learning l...  \n",
                        "\n",
                        "[12610 rows x 16 columns]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "for col in data.columns:\r\n",
                "    print(col)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Unnamed: 0\n",
                        "subreddit\n",
                        "subreddit_subscribers\n",
                        "title\n",
                        "id\n",
                        "author\n",
                        "created_utc\n",
                        "num_comments\n",
                        "score\n",
                        "selftext\n",
                        "url\n",
                        "upvote_ratio\n",
                        "comments\n",
                        "clean_title\n",
                        "clean_selftext\n",
                        "clean_comments\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "data.to_csv(r\"C:\\\\Users\\\\20jam\\\\Documents\\\\github\\\\my-code\\\\clean_fulldata.csv\", index = False, header = True)\r\n",
                "data = data.drop(columns = [\"title\",\"selftext\", \"comments\"])\r\n",
                "data.to_csv(r\"C:\\\\Users\\\\20jam\\\\Documents\\\\github\\\\my-code\\\\fulldata_cleantext.csv\", index = False, header = True)"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.4",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.4 64-bit"
        },
        "interpreter": {
            "hash": "bf3e9a0eaed196172b25c3c8d2e3c0605dffe77010244cc0729cd883a8cd1a4e"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}