{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "# read in data\r\n",
                "import pandas as pd\r\n",
                "data = pd.read_csv(\"C:\\\\Users\\\\20jam\\\\Documents\\\\github\\\\my-code\\\\full_data.csv\") # do this for full_data, midCOVID_data, preCOVID_data\r\n",
                "title = data.loc[:, \"title\"]\r\n",
                "body = data.loc[:, \"selftext\"]\r\n",
                "comments = data.loc[:, \"comments\"]\r\n",
                "# print(title)\r\n",
                "# print(body)\r\n",
                "# print(comments)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "# lower every case\r\n",
                "lower_title = [item.lower() for item in title]\r\n",
                "lower_body = [item.lower() for item in body]\r\n",
                "lower_comments = []\r\n",
                "for item in comments:\r\n",
                "    try:\r\n",
                "        lower_comments.append(item.lower())\r\n",
                "    except:\r\n",
                "        lower_comments.append(\"\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "# remove stopwords\r\n",
                "import nltk\r\n",
                "from nltk.corpus import stopwords\r\n",
                "nltk.download(\"stopwords\")\r\n",
                "stop_words = set(stopwords.words(\"english\"))\r\n",
                "print(stop_words)\r\n",
                "\r\n",
                "stop_title = []\r\n",
                "for text in lower_title:\r\n",
                "    text = \" \".join([word for word in text.split() if word not in stop_words])\r\n",
                "    stop_title.append(text)\r\n",
                "stop_body = []\r\n",
                "for text in lower_body:\r\n",
                "    text = \" \".join([word for word in text.split() if word not in stop_words])\r\n",
                "    stop_body.append(text)\r\n",
                "stop_comments = []\r\n",
                "for text in lower_comments:\r\n",
                "    text = \" \".join([word for word in text.split() if word not in stop_words])\r\n",
                "    stop_comments.append(text)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "[nltk_data] Downloading package stopwords to\n",
                        "[nltk_data]     C:\\Users\\20jam\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "{\"couldn't\", 'hasn', 'don', 'my', 'on', 'not', 'having', 'most', 'hers', \"it's\", \"mustn't\", \"you've\", \"that'll\", 'themselves', \"shan't\", 'as', 'him', \"should've\", 'was', 'are', 'wouldn', 'after', 'where', 'should', 'than', \"isn't\", 'few', 'same', 'while', 'does', 'if', 'were', 'has', 'and', \"aren't\", \"don't\", 'he', 'didn', 'needn', 'own', \"wouldn't\", 'ours', 'both', 'our', 'been', 'their', 'to', 'll', 'they', 'any', 'haven', 'will', \"shouldn't\", \"mightn't\", \"she's\", 'out', 'your', 'yourself', 'just', 'all', 'had', 'under', 'into', 'through', 'down', 'have', 'why', 'between', \"won't\", 'nor', 'each', 'we', 'but', 't', 'isn', 'her', 's', 'me', 'how', 'did', 'from', 'what', 'in', 'y', 'about', 'can', 'ma', \"weren't\", 'against', 'be', 'this', \"you'd\", 'them', 'more', \"hadn't\", 'is', 'mightn', 'before', 'mustn', 'no', 'an', 'whom', 'd', 'myself', 'doesn', 'do', 'it', 'a', \"needn't\", 'for', 'further', 'by', 'over', 'theirs', 'up', 'during', 'some', 'am', 'so', 'the', \"wasn't\", 'i', 'then', \"hasn't\", 'o', 'with', \"haven't\", 'ourselves', 'shan', 'she', 'there', 'which', 'yours', 'who', 'very', 'these', 'aren', 'won', 'when', \"you'll\", \"you're\", \"doesn't\", 'its', 'being', 'or', 'off', 'herself', 'because', 'only', 'couldn', 'again', 've', 'wasn', 'here', 'other', 'at', 'too', 'those', 'above', 'of', 're', 'shouldn', \"didn't\", 'm', 'weren', 'ain', 'himself', 'now', 'until', 'you', 'once', 'hadn', 'his', 'itself', 'such', 'below', 'doing', 'yourselves', 'that'}\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "# expand contractions\r\n",
                "import contractions\r\n",
                "expanded_title = []\r\n",
                "for text in title:\r\n",
                "    expanded = []\r\n",
                "    for word in text.split():\r\n",
                "        expanded.append(contractions.fix(word))\r\n",
                "    expanded_text = \" \".join(expanded)\r\n",
                "    expanded_title.append(expanded_text)\r\n",
                "expanded_body = []\r\n",
                "for text in body:\r\n",
                "    expanded = []\r\n",
                "    for word in text.split():\r\n",
                "        expanded.append(contractions.fix(word))\r\n",
                "    expanded_text = \" \".join(expanded)\r\n",
                "    expanded_body.append(expanded_text)\r\n",
                "expanded_comments = []\r\n",
                "for text in comments:\r\n",
                "    try:\r\n",
                "        expanded = []\r\n",
                "        for word in text.split():\r\n",
                "            expanded.append(contractions.fix(word))\r\n",
                "        expanded_text = \" \".join(expanded)\r\n",
                "        expanded_comments.append(expanded_text)\r\n",
                "    except: # comments is NAN\r\n",
                "        expanded_comments.append(\"\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "# remove unicode\r\n",
                "import string\r\n",
                "unicode_title = []\r\n",
                "for text in stop_title:\r\n",
                "    text_encode = text.encode(encoding = \"ascii\", errors = \"ignore\")\r\n",
                "    text_decode = text_encode.decode()\r\n",
                "    clean_text = \" \".join([word for word in text_decode.split()]) # remove extra whitespace\r\n",
                "    unicode_title.append(clean_text)\r\n",
                "unicode_body = []\r\n",
                "for text in stop_body:\r\n",
                "    text_encode = text.encode(encoding = \"ascii\", errors = \"ignore\")\r\n",
                "    text_decode = text_encode.decode()\r\n",
                "    clean_text = \" \".join([word for word in text_decode.split()]) # remove extra whitespace\r\n",
                "    unicode_body.append(clean_text)\r\n",
                "unicode_comments = []\r\n",
                "for text in stop_comments:\r\n",
                "    text_encode = text.encode(encoding = \"ascii\", errors = \"ignore\")\r\n",
                "    text_decode = text_encode.decode()\r\n",
                "    clean_text = \" \".join([word for word in text_decode.split()]) # remove extra whitespace\r\n",
                "    unicode_comments.append(clean_text)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "# remove mentions, money sign, url, hashtags, punctuations, digits\r\n",
                "import re\r\n",
                "import string\r\n",
                "punct = set(string.punctuation)\r\n",
                "\r\n",
                "remove_title = []\r\n",
                "for text in unicode_title:\r\n",
                "    text = re.sub(\"@\\S+\", \"\", text)\r\n",
                "    text = re.sub(\"\\$\", \"\", text)\r\n",
                "    text = re.sub(r'http\\S+', \"\", text)\r\n",
                "    text = re.sub(\"#\", \"\", text)\r\n",
                "    text = \"\".join([ch for ch in text if ch not in punct])\r\n",
                "    text = re.sub(r'[0-9]', \"\", text)\r\n",
                "    remove_title.append(text)\r\n",
                "remove_body = []\r\n",
                "for text in unicode_body:\r\n",
                "    text = re.sub(\"@\\S+\", \"\", text)\r\n",
                "    text = re.sub(\"\\$\", \"\", text)\r\n",
                "    text = re.sub(r'http\\S+', \"\", text)\r\n",
                "    text = re.sub(\"#\", \"\", text)\r\n",
                "    text = \"\".join([ch for ch in text if ch not in punct])\r\n",
                "    text = re.sub(r'[0-9]', \"\", text)\r\n",
                "    remove_body.append(text)\r\n",
                "remove_comments = []\r\n",
                "for text in unicode_comments:\r\n",
                "    text = re.sub(\"@\\S+\", \"\", text)\r\n",
                "    text = re.sub(\"\\$\", \"\", text)\r\n",
                "    text = re.sub(r'http\\S+', \"\", text)\r\n",
                "    text = re.sub(\"#\", \"\", text)\r\n",
                "    text = \"\".join([ch for ch in text if ch not in punct])\r\n",
                "    text = re.sub(r'[0-9]', \"\", text)\r\n",
                "    remove_comments.append(text)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "# lemmatize\r\n",
                "import nltk\r\n",
                "from nltk.stem import WordNetLemmatizer\r\n",
                "lemmatizer = WordNetLemmatizer()\r\n",
                "\r\n",
                "lemma_title = []\r\n",
                "for text in remove_title:\r\n",
                "    lemmatized = [lemmatizer.lemmatize(word) for word in text.split()]\r\n",
                "    text = \" \".join(word for word in lemmatized)\r\n",
                "    lemma_title.append(text)\r\n",
                "lemma_body = []\r\n",
                "for text in remove_body:\r\n",
                "    lemmatized = [lemmatizer.lemmatize(word) for word in text.split()]\r\n",
                "    text = \" \".join(word for word in lemmatized)\r\n",
                "    lemma_body.append(text)\r\n",
                "lemma_comments = []\r\n",
                "for text in remove_comments:\r\n",
                "    lemmatized = [lemmatizer.lemmatize(word) for word in text.split()]\r\n",
                "    text = \" \".join(word for word in lemmatized)\r\n",
                "    lemma_comments.append(text)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "print(len(lemma_title))\r\n",
                "print(len(lemma_body))\r\n",
                "print(len(lemma_comments))\r\n",
                "data = data.drop(columns = [\"title\",\"selftext\", \"comments\"])\r\n",
                "# data[\"clean_title\"] = lemma_title\r\n",
                "# data[\"clean_selftext\"] = lemma_body\r\n",
                "# data[\"clean_comments\"] = lemma_comments\r\n",
                "print(data)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "12610\n",
                        "12610\n",
                        "12610\n",
                        "       Unnamed: 0          subreddit  subreddit_subscribers      id  \\\n",
                        "0               2       askSingapore                  44675  ng6k8b   \n",
                        "1               4  NationalServiceSG                  11091  mfnwal   \n",
                        "2               5       askSingapore                  44675  l0ac4u   \n",
                        "3               7            SGExams                  77716  kzrgon   \n",
                        "4               8  NationalServiceSG                  11091  kwbxjs   \n",
                        "...           ...                ...                    ...     ...   \n",
                        "12605       84031          singapore                 231401  g81nb4   \n",
                        "12606       84051          singapore                 159359  b0hzfi   \n",
                        "12607       84081          singapore                 154905  ap5vn0   \n",
                        "12608       84101          singapore                 195542  dopbox   \n",
                        "12609       84121          singapore                 144499  a21gtg   \n",
                        "\n",
                        "                   author created_utc  num_comments  score  \\\n",
                        "0            summerfellxx  2021-05-19             6      8   \n",
                        "1            ElijahThor00  2021-03-29             6      5   \n",
                        "2           kanicroquette  2021-01-19             9     42   \n",
                        "3         Vivid-Wing-3460  2021-01-18             7     44   \n",
                        "4       chin_chin_daisuki  2021-01-13             8     21   \n",
                        "...                   ...         ...           ...    ...   \n",
                        "12605  QualitativeEconomy  2020-04-25             0      2   \n",
                        "12606          wintersoju  2019-03-13             8      7   \n",
                        "12607           yummydubu  2019-02-10            20      1   \n",
                        "12608   jasonrodriguez_DT  2019-10-29             6      0   \n",
                        "12609                 NaN  2018-12-01           117      1   \n",
                        "\n",
                        "                                                     url  upvote_ratio  \\\n",
                        "0      https://www.reddit.com/r/askSingapore/comments...          0.84   \n",
                        "1      https://www.reddit.com/r/NationalServiceSG/com...          0.86   \n",
                        "2      https://www.reddit.com/r/askSingapore/comments...          1.00   \n",
                        "3      https://www.reddit.com/r/SGExams/comments/kzrg...          0.95   \n",
                        "4      https://www.reddit.com/r/NationalServiceSG/com...          0.90   \n",
                        "...                                                  ...           ...   \n",
                        "12605  https://www.reddit.com/r/singapore/comments/g8...          0.54   \n",
                        "12606  https://www.reddit.com/r/singapore/comments/b0...          0.71   \n",
                        "12607  https://www.reddit.com/r/singapore/comments/ap...          0.73   \n",
                        "12608  https://www.reddit.com/r/singapore/comments/do...          0.56   \n",
                        "12609  https://www.reddit.com/r/singapore/comments/a2...          0.95   \n",
                        "\n",
                        "                                             clean_title  \\\n",
                        "0      anyone know get tested adult auditory processi...   \n",
                        "1                               help adjustment disorder   \n",
                        "2                           get assessed eating disorder   \n",
                        "3                                   rant mental disorder   \n",
                        "4                            downpes adjustment disorder   \n",
                        "...                                                  ...   \n",
                        "12605                                 vaccine whats next   \n",
                        "12606                                  influenza vaccine   \n",
                        "12607  lady hpv vaccination yn experience cost medisa...   \n",
                        "12608  planning go back sg december polio vaccine req...   \n",
                        "12609                         got scammed mlm lost money   \n",
                        "\n",
                        "                                          clean_selftext  \\\n",
                        "0      know mostly test child really need know whats ...   \n",
                        "1      people say half war report mental illness long...   \n",
                        "2      might eating disorder dont know dont want self...   \n",
                        "3      o long period time sit really reflect unsolvab...   \n",
                        "4      facing severe adjustment problem mental breakd...   \n",
                        "...                                                  ...   \n",
                        "12605  tbh havent kept vaccine development news want ...   \n",
                        "12606  different brand vaccine available difference g...   \n",
                        "12607  currently researching vaccination wondering ma...   \n",
                        "12608  checked couple site including ministry health ...   \n",
                        "12609  lost mlm company called icsi care system feel ...   \n",
                        "\n",
                        "                                          clean_comments  \n",
                        "0      is recent going since childhood nnif recent mi...  \n",
                        "1      pretty long unfortunately im currently going c...  \n",
                        "2      dont self diagnose cheapest way got polyclinic...  \n",
                        "3      you sound exactly like me tell thought cant si...  \n",
                        "4      upvote chinchindaisuki it take month fast mont...  \n",
                        "...                                                  ...  \n",
                        "12605  whats stopping anyone hoarding vaccine it made...  \n",
                        "12606            yes difference co many strain influenza  \n",
                        "12607  yes claim medisave four strand one already sec...  \n",
                        "12608  not sure official requirement ive seen many fr...  \n",
                        "12609  learning part lifenntreat expensive learning l...  \n",
                        "\n",
                        "[12610 rows x 13 columns]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "for col in data.columns:\r\n",
                "    print(col)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Unnamed: 0\n",
                        "subreddit\n",
                        "subreddit_subscribers\n",
                        "id\n",
                        "author\n",
                        "created_utc\n",
                        "num_comments\n",
                        "score\n",
                        "url\n",
                        "upvote_ratio\n",
                        "clean_title\n",
                        "clean_selftext\n",
                        "clean_comments\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "data.to_csv(r\"C:\\\\Users\\\\20jam\\\\Documents\\\\github\\\\my-code\\\\fulldata_cleantext.csv\", index = False, header = True)"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}