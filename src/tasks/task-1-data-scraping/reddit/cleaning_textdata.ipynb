{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 93,
            "source": [
                "# read in data\r\n",
                "import pandas as pd\r\n",
                "data = pd.read_csv(\"C:\\\\Users\\\\20jam\\\\Documents\\\\a teabag of joy\\\\data cleaning\\\\full_data.csv\")\r\n",
                "title = data.loc[:, \"title\"]\r\n",
                "body = data.loc[:, \"selftext\"]\r\n",
                "comments = data.loc[:, \"comments\"]\r\n",
                "print(title)\r\n",
                "print(body)\r\n",
                "print(comments)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "0        Anyone know where to get tested as an Adult fo...\n",
                        "1                             Help for adjustment disorder\n",
                        "2              Where to get assessed for eating disorders?\n",
                        "3                                  [RANT] Mental disorders\n",
                        "4                         Downpes from adjustment disorder\n",
                        "                               ...                        \n",
                        "12607                   If there is a vaccine, whats next?\n",
                        "12608                                    Influenza Vaccine\n",
                        "12609    [ladies] HPV vaccination - y/n, experience, co...\n",
                        "12610    Planning to go back to SG this December. Polio...\n",
                        "12611    Just got scammed by MLM and lost money, what t...\n",
                        "Name: title, Length: 12612, dtype: object\n",
                        "0        I know mostly these tests are for children. Bu...\n",
                        "1        People say half the war is won when you report...\n",
                        "2        I might have an eating disorder, I don’t know ...\n",
                        "3        After o's I have this long period of time to s...\n",
                        "4        Facing severe adjustment problems, had a menta...\n",
                        "                               ...                        \n",
                        "12607    Tbh I havent kept up with the vaccine developm...\n",
                        "12608    Are there different brands of the vaccine avai...\n",
                        "12609    Currently researching the vaccination and wond...\n",
                        "12610    I checked a couple of sites including your Min...\n",
                        "12611    I just lost $1688 to an MLM company called ICS...\n",
                        "Name: selftext, Length: 12612, dtype: object\n",
                        "0        [\"is this recent or has it been going on since...\n",
                        "1        ['pretty long unfortunately. :( i’m currently ...\n",
                        "2        [\"Don't self diagnose. Cheapest way: got to po...\n",
                        "3        [\"You sound exactly like me. What I can tell y...\n",
                        "4        ['upvote for chin_chin_daisuki', 'It can take ...\n",
                        "                               ...                        \n",
                        "12607    ['What's stopping anyone from hoarding a vacci...\n",
                        "12608    ['Yes there are differences cos many strains o...\n",
                        "12609    ['yes you can claim from medisave for the four...\n",
                        "12610    ['Not sure on what is the 'official requiremen...\n",
                        "12611    ['Learning is part of life Treat that $1688 as...\n",
                        "Name: comments, Length: 12612, dtype: object\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 99,
            "source": [
                "# lower every case\r\n",
                "lower_title = [item.lower() for item in title]\r\n",
                "lower_body = [item.lower() for item in body]\r\n",
                "lower_comments = []\r\n",
                "for item in comments:\r\n",
                "    try:\r\n",
                "        lower_comments.append(item.lower())\r\n",
                "    except:\r\n",
                "        lower_comments.append(\"\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 100,
            "source": [
                "# remove stopwords\r\n",
                "import nltk\r\n",
                "from nltk.corpus import stopwords\r\n",
                "nltk.download(\"stopwords\")\r\n",
                "stop_words = set(stopwords.words(\"english\"))\r\n",
                "print(stop_words)\r\n",
                "\r\n",
                "stop_title = []\r\n",
                "for text in lower_title:\r\n",
                "    text = \" \".join([word for word in text.split() if word not in stop_words])\r\n",
                "    stop_title.append(text)\r\n",
                "stop_body = []\r\n",
                "for text in lower_body:\r\n",
                "    text = \" \".join([word for word in text.split() if word not in stop_words])\r\n",
                "    stop_body.append(text)\r\n",
                "stop_comments = []\r\n",
                "for text in lower_comments:\r\n",
                "    text = \" \".join([word for word in text.split() if word not in stop_words])\r\n",
                "    stop_comments.append(text)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "[nltk_data] Downloading package stopwords to\n",
                        "[nltk_data]     C:\\Users\\20jam\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "{'ourselves', 'was', 'so', 'those', \"hasn't\", 'as', 'himself', 'didn', \"shouldn't\", 'while', 'hadn', 'from', 'is', \"that'll\", 'just', 'isn', 'ma', 'down', 'only', 'doing', \"it's\", 'by', 'that', 'do', 'how', 'on', \"mightn't\", \"won't\", \"don't\", 'off', 'out', 'further', 'not', 'between', 'them', \"should've\", 'same', \"you'd\", \"doesn't\", 'her', 'then', 'can', \"couldn't\", 'our', 'any', 'his', 'did', 'if', 'have', 'm', 't', 'him', 'shouldn', 'where', \"you're\", 'under', 'now', 'their', 'wouldn', 'which', 'you', 'should', 've', 'had', 'than', 'ours', \"you've\", 'but', 'me', \"wouldn't\", 'below', \"shan't\", \"isn't\", 'she', 'were', 'some', 'yourselves', 'it', \"mustn't\", \"weren't\", 'too', \"hadn't\", 'over', 'o', 'll', 'more', 'shan', 's', 'yourself', 'he', 'in', 'myself', 'an', \"haven't\", 'very', 'through', 'mightn', 'yours', 'here', 'of', 'again', 'does', \"you'll\", 'herself', 'd', 'because', 'theirs', 'we', 'having', 'during', 'wasn', 'into', 're', 'who', 'no', 'own', 'the', 'am', 'i', 'been', 'they', 'don', 'for', 'at', 'y', 'my', 'itself', 'will', 'doesn', 'being', 'nor', 'mustn', 'aren', \"she's\", \"wasn't\", 'after', 'against', 'hasn', 'above', 'such', 'its', 'couldn', 'until', 'your', 'or', 'haven', 'with', 'other', \"needn't\", 'has', 'few', \"aren't\", 'weren', 'what', 'up', 'to', 'when', 'hers', 'ain', 'once', 'both', 'there', 'all', 'before', 'most', 'needn', 'this', 'are', 'a', 'be', 'and', 'why', 'these', 'each', 'about', 'themselves', 'whom', 'won', \"didn't\"}\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 101,
            "source": [
                "# expand contractions\r\n",
                "import contractions\r\n",
                "expanded_title = []\r\n",
                "for text in title:\r\n",
                "    expanded = []\r\n",
                "    for word in text.split():\r\n",
                "        expanded.append(contractions.fix(word))\r\n",
                "    expanded_text = \" \".join(expanded)\r\n",
                "    expanded_title.append(expanded_text)\r\n",
                "expanded_body = []\r\n",
                "for text in body:\r\n",
                "    expanded = []\r\n",
                "    for word in text.split():\r\n",
                "        expanded.append(contractions.fix(word))\r\n",
                "    expanded_text = \" \".join(expanded)\r\n",
                "    expanded_body.append(expanded_text)\r\n",
                "expanded_comments = []\r\n",
                "for text in comments:\r\n",
                "    try:\r\n",
                "        expanded = []\r\n",
                "        for word in text.split():\r\n",
                "            expanded.append(contractions.fix(word))\r\n",
                "        expanded_text = \" \".join(expanded)\r\n",
                "        expanded_comments.append(expanded_text)\r\n",
                "    except: # comments is NAN\r\n",
                "        expanded_comments.append(\"\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 102,
            "source": [
                "# remove unicode\r\n",
                "import string\r\n",
                "unicode_title = []\r\n",
                "for text in stop_title:\r\n",
                "    text_encode = text.encode(encoding = \"ascii\", errors = \"ignore\")\r\n",
                "    text_decode = text_encode.decode()\r\n",
                "    clean_text = \" \".join([word for word in text_decode.split()]) # remove extra whitespace\r\n",
                "    unicode_title.append(clean_text)\r\n",
                "unicode_body = []\r\n",
                "for text in stop_body:\r\n",
                "    text_encode = text.encode(encoding = \"ascii\", errors = \"ignore\")\r\n",
                "    text_decode = text_encode.decode()\r\n",
                "    clean_text = \" \".join([word for word in text_decode.split()]) # remove extra whitespace\r\n",
                "    unicode_body.append(clean_text)\r\n",
                "unicode_comments = []\r\n",
                "for text in stop_comments:\r\n",
                "    text_encode = text.encode(encoding = \"ascii\", errors = \"ignore\")\r\n",
                "    text_decode = text_encode.decode()\r\n",
                "    clean_text = \" \".join([word for word in text_decode.split()]) # remove extra whitespace\r\n",
                "    unicode_comments.append(clean_text)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 103,
            "source": [
                "# remove mentions, money sign, url, hashtags, punctuations, digits\r\n",
                "import re\r\n",
                "import string\r\n",
                "punct = set(string.punctuation)\r\n",
                "\r\n",
                "remove_title = []\r\n",
                "for text in unicode_title:\r\n",
                "    text = re.sub(\"@\\S+\", \"\", text)\r\n",
                "    text = re.sub(\"\\$\", \"\", text)\r\n",
                "    text = re.sub(r'http\\S+', \"\", text)\r\n",
                "    text = re.sub(\"#\", \"\", text)\r\n",
                "    text = \"\".join([ch for ch in text if ch not in punct])\r\n",
                "    text = re.sub(r'[0-9]', \"\", text)\r\n",
                "    remove_title.append(text)\r\n",
                "remove_body = []\r\n",
                "for text in unicode_body:\r\n",
                "    text = re.sub(\"@\\S+\", \"\", text)\r\n",
                "    text = re.sub(\"\\$\", \"\", text)\r\n",
                "    text = re.sub(r'http\\S+', \"\", text)\r\n",
                "    text = re.sub(\"#\", \"\", text)\r\n",
                "    text = \"\".join([ch for ch in text if ch not in punct])\r\n",
                "    text = re.sub(r'[0-9]', \"\", text)\r\n",
                "    remove_body.append(text)\r\n",
                "remove_comments = []\r\n",
                "for text in unicode_comments:\r\n",
                "    # text = re.sub(\"@\\S+\", \"\", text)\r\n",
                "    # text = re.sub(\"\\$\", \"\", text)\r\n",
                "    text = re.sub(r'http\\S+', \"\", text)\r\n",
                "    text = re.sub(\"#\", \"\", text)\r\n",
                "    text = \"\".join([ch for ch in text if ch not in punct])\r\n",
                "    text = re.sub(r'[0-9]', \"\", text)\r\n",
                "    remove_comments.append(text)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 107,
            "source": [
                "# lemmatize\r\n",
                "import nltk\r\n",
                "from nltk.stem import WordNetLemmatizer\r\n",
                "lemmatizer = WordNetLemmatizer()\r\n",
                "\r\n",
                "lemma_title = []\r\n",
                "for text in remove_title:\r\n",
                "    lemmatized = [lemmatizer.lemmatize(word) for word in text.split()]\r\n",
                "    text = \" \".join(word for word in lemmatized)\r\n",
                "    lemma_title.append(text)\r\n",
                "lemma_body = []\r\n",
                "for text in remove_body:\r\n",
                "    lemmatized = [lemmatizer.lemmatize(word) for word in text.split()]\r\n",
                "    text = \" \".join(word for word in lemmatized)\r\n",
                "    lemma_body.append(text)\r\n",
                "lemma_comments = []\r\n",
                "for text in remove_comments:\r\n",
                "    lemmatized = [lemmatizer.lemmatize(word) for word in text.split()]\r\n",
                "    text = \" \".join(word for word in lemmatized)\r\n",
                "    lemma_comments.append(text)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 110,
            "source": [
                "print(len(lemma_title))\r\n",
                "print(len(lemma_body))\r\n",
                "print(len(lemma_comments))\r\n",
                "data.drop(columns = [\"title\",\"selftext\", \"comments\"])\r\n",
                "data[\"title\"] = lemma_title\r\n",
                "data[\"selftext\"] = lemma_body\r\n",
                "data[\"comments\"] = lemma_comments\r\n",
                "print(data)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "12612\n",
                        "12612\n",
                        "12612\n",
                        "       Unnamed: 0  ...                                           comments\n",
                        "0               2  ...  is recent going since childhood nnif recent mi...\n",
                        "1               4  ...  pretty long unfortunately im currently going c...\n",
                        "2               5  ...  dont self diagnose cheapest way got polyclinic...\n",
                        "3               7  ...  you sound exactly like me tell thought cant si...\n",
                        "4               8  ...  upvote chinchindaisuki it take month fast mont...\n",
                        "...           ...  ...                                                ...\n",
                        "12607        8552  ...  whats stopping anyone hoarding vaccine made di...\n",
                        "12608       85541  ...            yes difference co many strain influenza\n",
                        "12609       85571  ...  yes claim medisave four strand one already sec...\n",
                        "12610       85591  ...  not sure official requirement ive seen many fr...\n",
                        "12611        8561  ...  learning part life treat expensive learning le...\n",
                        "\n",
                        "[12612 rows x 13 columns]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 111,
            "source": [
                "for col in data.columns:\r\n",
                "    print(col)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Unnamed: 0\n",
                        "subreddit\n",
                        "subreddit_subscribers\n",
                        "title\n",
                        "id\n",
                        "author\n",
                        "created_utc\n",
                        "num_comments\n",
                        "score\n",
                        "selftext\n",
                        "url\n",
                        "upvote_ratio\n",
                        "comments\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 112,
            "source": [
                "data.to_csv(r\"C:\\\\Users\\\\20jam\\\\Documents\\\\github\\\\my-code\\\\fulldata_cleantext.csv\", index = False, header = True)"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}