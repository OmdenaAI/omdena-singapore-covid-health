{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fc98862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string \n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = list(get_stop_words('en'))\n",
    "nltk_words = list(stopwords.words('english'))\n",
    "stop_words.extend(nltk_words)\n",
    "from contraction_map import CONTRACTION_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2c5c933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82149, 7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop some features\n",
    "data = pd.read_csv(\"2021-07-12-tweets-twint.csv\")\n",
    "data.head()\n",
    "cols = ['tweet_id', 'date', 'clean_tweet', 'hashtags', \n",
    "            'search_keyword', 'year', 'month']\n",
    "#data.loc[5:10,['clean_tweet', 'year']]\n",
    "data = data.loc[:,cols]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa068268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean thoroughly \n",
    "def remove_hyperlinks(text):\n",
    "    ptn = r'(https://[\\w./-]+)|(www.[\\w./-]+)|([\\w./-]+.com)'\n",
    "    return re.sub(ptn, '', text)\n",
    "\n",
    "def remove_mentions(text):\n",
    "    ptn = r'(@[\\w_]+ | (@[.]+) | (@))'\n",
    "    return re.sub(ptn, '', text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    #text = list(text)\n",
    "    text=' '.join([x for x in text.split() if x not in stop_words])\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(text): \n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text) \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    #text = text.translate(None, string.punctuation)\n",
    "    return text \n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "    \n",
    "def remove_alphabets(text):\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "    return text \n",
    "\n",
    "def remove_digits(text):\n",
    "    text = ''.join(i for i in text if not i.isdigit())\n",
    "    return text\n",
    "\n",
    "def clean_text(text, hyperlink=True, mention=True, stopwords=True,\n",
    "               punctuations=True, contractions=True, digits=True,\n",
    "               lowercase=True, alphabets=True):\n",
    "    if lowercase: # Transform to lowercase\n",
    "        text = text.lower()\n",
    "    if hyperlink: # Remove Hyperlinks\n",
    "        text = remove_hyperlinks(text)\n",
    "    if mention: # Remove Mentions\n",
    "        text = remove_mentions(text)\n",
    "    if punctuations: # Remove Punctuations \n",
    "        text = remove_punctuations(text) \n",
    "    if contractions: # Expand Contractions e.g. can't -> cannot\n",
    "        text = expand_contractions(text)\n",
    "    if stopwords: # Remove english stopwords\n",
    "        text = remove_stopwords(text)\n",
    "    if alphabets: # Remove single alphabets \n",
    "        text = remove_alphabets(text)\n",
    "    if digits: # Remove all numbers\n",
    "        text = remove_digits(text)\n",
    "    return text\n",
    "\n",
    "data['clean_tweet3'] = data.clean_tweet.map(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a57d442",
   "metadata": {},
   "source": [
    "### Text lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84efd9cb",
   "metadata": {},
   "source": [
    " <strong>Lemmatization</strong> usually refers to doing things properly with the use of a vocabulary and morphological\n",
    "analysis of words, normally aiming to remove inflectional endings only and to return the base or \n",
    "dictionary form of a word, which is known as the lemma <br><br>\n",
    "\n",
    "Source: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/ <br>\n",
    "am, are, is $\\Rightarrow$ be <br>\n",
    "car, cars, car's, cars' $\\Rightarrow$ car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6168d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install spacy \n",
    "# !pip3 install -U spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a89119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import nltk\n",
    "#nlp = spacy.load('en_core')\n",
    "#nlp_vec = spacy.load('en_vecs', parse = True, tag=True, entity=True)\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea782924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = str(text) #Type error (consist of float type..)\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "data[\"parse_tweet\"] = dt.clean_tweet3.map(lemmatize_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}