{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and importing libraries\n",
    "If you have not installed beautifulsoup and requests before, uncomment the first 2 lines to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install beautifulsoup4\n",
    "# ! pip install requests\n",
    "\n",
    "import urllib.request,sys,time\n",
    "# from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a function to scrape the URLs\n",
    "\n",
    "The main url we are scraping from is https://www.channelnewsasia.com/news/topic/mental-health?pageNum=0, where articles tagged as \"mental health\" will appear under the url. We will interate through the different pages of the url by changing the ending page number.\n",
    "\n",
    "The first page of the articles from the url under the mental health tag is 0, and the last page is 23.\n",
    "We will create a function that allows us to loop through the pages using the pageNum parameter.\n",
    "\n",
    "Please update the headers variable with your own user agent. This prevents us from running into error 403: Forbidden when we scrape the URLs. \n",
    "\n",
    "\n",
    "To do so, \n",
    "1. Press F12 to navigate to the Chrome developer console.\n",
    "2. Type in <code>navigator.userAgent</code> in the console and execute it by hitting enter.\n",
    "3. Copy over your user agent and replace the value with your own in the headers dictionary.\n",
    "\n",
    "For more details, refer to https://stackoverflow.com/questions/38489386/python-requests-403-forbidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(starting_page, ending_page):\n",
    "    \n",
    "    #rename csv as name_urls.csv\n",
    "    with open('jhinyee_urls.csv', 'w', newline='') as file: #create a csv to input scrapped urls\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Page\", \"URL\"]) #create first header row with the column names as \"Page\" to indicate page number and \"URL\"\n",
    "    \n",
    "\n",
    "        for i in range(starting_page, ending_page+1):\n",
    "            try:\n",
    "                url = 'https://www.channelnewsasia.com/news/topic/mental-health?pageNum='+str(i)\n",
    "                headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "                \n",
    "                result = requests.get(url, headers=headers)\n",
    "\n",
    "                soup=BeautifulSoup(result.text,'html.parser')\n",
    "\n",
    "                article_urls = soup.findAll('a', attrs={'class' : 'teaser__title'}) #article link found under class teaser__title\n",
    "                for link in article_urls:\n",
    "                    writer.writerow([i, 'https://www.channelnewsasia.com'+link['href']]) #add row in csv with the page number and scrapped url\n",
    "                                                                                        #link['href'] only gives us the relative path, not the absolute path, so we need to add the missing domain\n",
    "                                                                                        #examples of link['href']: /news/sport/tennis--athletes-are-humans---osaka-stands-by-decision-to-skip-media-duties-15181690\n",
    "\n",
    "            except Exception as e:\n",
    "                print('exception')\n",
    "                error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "                print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "                print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "                continue                                              #ignore this page. Abandon this and go back.\n",
    "\n",
    "            time.sleep(2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_urls(0,23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing around with the beautifulsoup code\n",
    "\n",
    "The url provided in the cell below is a page of articles tagged under CNA's mental health tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/news/sport/tennis--athletes-are-humans---osaka-stands-by-decision-to-skip-media-duties-15181690\n",
      "/news/singapore/suicide-highest-record-elderly-mental-health-isolation-covid-19-15179528\n",
      "/news/lifestyle/mental-health-kids-suicide-15120448\n",
      "/news/singapore/ai-in-mental-health-screening-system-enables-early-detection-of-15126254\n",
      "/news/commentary/britney-spears-conservator-guardian-us-court-framing-documentary-15108750\n",
      "/news/commentary/music-eilish-spears-lovato-gaga-metoo-abuse-mental-health-media-15090460\n",
      "/news/commentary/night-owls-work-home-more-rest-morning-lark-revenge-bedtime-15082770\n",
      "/news/video-on-demand/asia-first-encore/cna-talking-point-discusses-keeping-kids-occupied-this-school-15077584\n",
      "/news/commentary/mental-health-challenges-work-from-home-employers-leave-15066440\n",
      "/news/commentary/revenge-bedtime-procrastination-sleep-difficulty-falling-asleep-14971486\n"
     ]
    }
   ],
   "source": [
    "#for if you wanna play around with beautifulsoup\n",
    "\n",
    "url = 'https://www.channelnewsasia.com/news/topic/mental-health?pageNum=0'\n",
    "headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "#to replace the value with your own user agent, execute navigator.userAgent in the Chrome developer console (right-click, inspect, navigate to console on the header)\n",
    "\n",
    "result = requests.get(url, headers=headers)\n",
    "# print(result.content.decode())\n",
    "\n",
    "soup=BeautifulSoup(result.text,'html.parser')\n",
    "# print(soup)\n",
    "# print(page.text)\n",
    "# frame=[]\n",
    "# for item in soup.find_all(attrs={'class': 'teaser__content-wrapper'}):\n",
    "\n",
    "#     for link in item.find_all('a'):\n",
    "#         print(link)\n",
    "#         print (link.get('href'))\n",
    "\n",
    "article_urls = soup.findAll('a', attrs={'class' : 'teaser__title'})\n",
    "for link in article_urls:\n",
    "    print (link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#was experimenting with a way to store the urls before deciding on using csv instead\n",
    "#learnt of another way to create the keys as the page numbers and store the urls inside using m1 instead of m2\n",
    "\n",
    "\n",
    "#m1\n",
    "#         urls_dict = {} #key: page number, value: list of urls in page number\n",
    "#                 urls_dict.setdefault(i,[]).append('https://www.channelnewsasia.com'+link['href'])\n",
    "#         return urls_dict\n",
    "\n",
    "#m2\n",
    "#             if i in urls_dict:\n",
    "#                 urls_dict[i].append(link)\n",
    "#             else:\n",
    "#                 urls_dict[i] = link\n",
    "#             print (link['href'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
