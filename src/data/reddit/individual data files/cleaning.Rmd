---
title: "Data Cleaning & Merge Dataframes for Reddit CSV"
author: "Tan Jamie"
date: "7/9/2021"
output: html_document
---

# Fundamental Data Cleaning
## Set Up
```{r setup}
# set directory to path folder containing all csv and this code file only
# ensure no other files are in the same folder
library(tidyverse)
library(dplyr)
library(config)
library(DBI)
library(RPostgres)
library(stringr)
library(stringi)
library(reprex)
library(dplyr)
library(plyr)
library(textclean)
library(mice)
```


## Read in All Data
```{r read in all data}
files_date <- c("neurosis,disorder,dependence.csv", "phobia, addiction.csv", "reddit_overwhelmed.csv", 
          "reddit_paranoia.csv", "reddit-5.csv", "socialize, help.csv", "stigma, self-harm.csv")
files_utc <- list.files(pattern = "*.csv")[!(list.files(pattern = "*.csv") %in% files_date)]
data_date <- ldply(.data = files_date, 
              .fun = read.csv,
              header = TRUE) %>%
         select("subreddit", "subreddit_subscribers", "title", "id", "author", 
                "created_utc", "num_comments", "score", "selftext", "url", "upvote_ratio", "comments")
data_utc <- ldply(.data = files_utc, 
              .fun = read.csv,
              header = TRUE) %>%
         select("subreddit", "subreddit_subscribers", "title", "id", "author", 
                "created_utc", "num_comments", "score", "selftext", "url", "upvote_ratio", "comments")
```


## Convert unix time to datetime
```{r convert unix time to datetime}
data_date$created_utc <- data_date$created_utc %>% 
                    as.Date()
data_utc$created_utc <- data_utc$created_utc %>% 
                    as.integer() %>%
                    as.POSIXct(., origin = "1970-01-01") %>%
                    as.Date()
data_utc <- filter(data_utc, !is.na(data_utc$created_utc))
```


## Primary data screening
```{r primary data screening}
data <- rbind(data_date, data_utc)
str(data) # gives hint about what to clean
```


## Ensure all data types are correct
```{r ensure all data types are correct}
# convert strings to character type
attach(data)
data$author <- as.character(author)
data$id <- as.character(id)
data$selftext <- as.character(selftext)
data$title <- as.character(title)
data$url <- as.character(url)
data$subreddit <- as.character(subreddit)
data$comments <- as.character(comments)

# convert numbers to integers
# entire column usually gets converted into factor if its not all numeric or integer
# data <- data[!is.na(as.numeric(as.character(data$col_name))),] coerced to NA - filter out rows with NA
data$subreddit_subscribers <- as.integer(subreddit_subscribers)
data$num_comments <- as.integer(num_comments)
data$score <- as.integer(score)
# convert ratio to numeric data type
data$upvote_ratio <- as.numeric(upvote_ratio)
```


## Filter out invalid data points
```{r filter out invalid data points}
# remove rows with duplicated id
data <- distinct(data, id, .keep_all = TRUE)
# remove rows with deleted entries (empty id or selftext)
data <- data[!(is.na(data$id)) &
            !(is.na(data$selftext)| data$selftext == "[removed]" | data$selftext == "[deleted]"),]

# 0 upvote_ratio don't make sense, if it doesn't exist, it shows NA
index <- data$upvote_ratio == as.numeric(0)
data$upvote_ratio[index] <- NA

# remaining NAs in upvote_ratio and author columns 
# insight found: 
# most NA are found in entries scraped using unofficial API
# so try re-scrape using official API for those entries

# filter entries with NA for author or upvote_ratio
index <- is.na(data$author) | is.na(data$upvote_ratio)
rescrape <- data[index,]
write.csv(rescrape, "C:\\Users\\20jam\\Documents\\GitHub\\omdena-singapore-covid-health\\src\\data\\reddit\\rescrape.csv")

# filter for remaining entries
remaining <- data[!index,]
done_rescrape <- read.csv("C:\\Users\\20jam\\Documents\\GitHub\\omdena-singapore-covid-health\\src\\data\\reddit\\done_rescrape.csv") %>%
  select("subreddit", "subreddit_subscribers", "title", "id", "author", 
                "created_utc", "num_comments", "score", "selftext", "url", "upvote_ratio", "comments")

# bind re-scraped entries with remaining entries
data <- rbind(remaining, done_rescrape)
# check num of NAs remaining for author or upvote_ratio
index <- is.na(data$author) | is.na(data$upvote_ratio)
# filter out rows with NA columns still
data <- filter(data, !is.na(data$author))
```


# Multiple imputation on upvote_ratio
```{r imputation on upvote_ratio NAs}
# will not proceed with imputation since upvote_ratio == NA < 0.6%
# delete entries with upvote_ratio == NA
data <- filter(data, !is.na(data$upvote_ratio))

# # ready and analyze data
# data2 <- data
# data2$subreddit <- as.factor(data2$subreddit)
# data2$author <- as.factor(data2$author)
# p_missing <- unlist(lapply(data2, function(x) sum(is.na(x))))/nrow(data)
# sort(p_missing[p_missing > 0], decreasing = TRUE)
# 
# # imputation
# impute <- mice(data2, maxit = 0) # run mice code with 0 iteration
# predMatrix <- impute$predictorMatrix # extract predictor matrix
# meth <- impute$method # extract methods of imputation
# 
# # set values of var = 0 to leave it out in process of imputation
# # should created_utc be left out? can imputation process take in date?
# predMatrix[, c("title")] <- 0
# predMatrix[, c("id")] <- 0
# predMatrix[, c("selftext")] <- 0
# predMatrix[, c("url")] <- 0
# predMatrix[, c("comments")] <- 0
# head(predMatrix)
# 
# # variables we dont want to impute on
# meth[c("subreddit", "subreddit_subscribers", "author", "num_comments", "score", "created_utc", "title", "id", "selftext", "url", "comments")] <- ""
# meth
# 
# # impute on data, create 5 datasets, use predMatrix as prediction matrix, dont print process
# impute2 <- mice(data2, maxit = 5, 
#              predictorMatrix = predMatrix, 
#              method = meth, print =  FALSE)
# 
# # run regression on each dataset and pool to est the avg regression coefficient and correct standard errors
# data2_long <- mice::complete(data2, action="long", include = TRUE)
# data2_long$upvote_ratio <- with(data2_long, as.numeric(data2_long$upvote_ratio))
# data2_long_mids<-as.mids(data2_long)
# fitimp <- with(data2_long_mids,
#                lm(upvote_ratio ~ subreddit + subreddit_subscribers + author + 
#                     created_utc + num_comments + score))
# final_model <- pool(fitimp)
# summary(final_model)
# # use model to make imputation on original dataset: data
# 
# # Helpful resource:
# # https://medium.com/@danberdov/dealing-with-missing-data-8b71cd819501
# # https://data.library.virginia.edu/getting-started-with-multiple-imputation-in-r/
```



## Remove comment contents column (2nd edit)
It seems this column will not be used by risk models or in eda, so we remove excess data
```{r remove comments column}
data <- select(data, -c("comments"))
```

## Convert Dataframes into CSV
```{r convert dataframes into csv}
# replace the paths respectively
# note that excel has 32k character limit in each cell, so this file will inevitably
# have some content spillovered to other cells from comments and selftext displayed
# but through reading it into r again, error seem to only show in excel
# crossed check with others - reading in again will not cause formating problem
write.csv(data, "C:\\Users\\20jam\\Documents\\GitHub\\omdena-singapore-covid-health\\src\\data\\reddit\\fulldata.csv")
```


fin